---
layout: post
title: "Notes on RecSys"
subtitle: 'This note is partially based on the survey paper: *Deep-Learning-based Recommender System: A Survey and New Perspectives*'
author: "Yiqiao Jin (Ahren)"
header-style: text
mathjax: true
tags:
  - Recommender Systems
  - Explainable Recommendation
---

### Why Recommender systems
* Recommender systems (RecSys) are vital in various information access systems to boost business and facilitate decision-making process
* RecSys are prevalent across numerous web domains such as e-commerce and media websites
* e.g. 80 percent of movies watched on Netflix came from recommendations. 60 percent of video clicks came from home page recommendation in YouTube.

#### Basic Concepts
* "latent factor" means useful but unexplainable embeddings
* Cross-domain Recommendation: The purpose is to avoid Information Cocoons in which RecSys only recommends content that users have demonstrated their interests
* Information Network: A network where each node represents an entity (e.g. user in a social network) and each link represents a relationship between entities (e.g. friendship).
  * Nodes/links may have attributes, labels, and weights.
  * Links may carry rich semantic information

### Input / Output
#### Input features
* user preferences
* item features
* user-item past interactions 
* additional information
  * temporal data (e.g., sequence-aware recommender)
  * spatial data (e.g., POI recommender)
* Side note: POI recommender systems learns users' implicit preferences according to users' interaction history

#### Output
* Generally, RecSys yield classifications, ratings and rankings of items
* Specifically, CTR, or the probability a user will click on a recommended item. In many RecSys, the items returned to a user should be ranked by estimated CTR


### Categorization of RecSys

#### 1. Collaborative Filtering (CF)

* learn from user-item historical interactions
  * explicit feedbacks, e.g. user's historical ratings
  * implicit feedback, e.g. browsing history
* relies only on past user behavior, e.g., previous transactions or product ratings, without requiring the creation of explicit profiles.
* The name is coined by the developers of Tapestry, the first recommender system.

#### Cons(-)

* CF suffers cold start problem, due to its inability to address the system's new products and users.
  * In this aspect, **content filtering** is superior
* CF suffers data sparsity issue. A small number of users and items have a large number of ratings

#### 2. Content-based recommendation
* based primarily on comparisons across items' and users' auxiliary information.

#### 3. Hybrid recommender system

### Methodology

#### Notations
* $M$: Number of users
* $N$: Number of items
* $R$: Interaction Matrix
* $\hat{R}$: Predicted interaction Matrix
* $r_{ui}$: Preference of user $u$ to item $i$
* $\hat{r}_{ui}$: Predicted score
* $\mathbf{r}^{(u)} = \{r^{u1}, ..., r^{uN}\}$: partially observed vector to represent each user $u$
* $\mathbf{r}^{(i)} = \{r^{1i}, ..., r^{Mi}\}$: partially observed vector to represent each user $u$
* $U \in \mathbb{R}^{M \times k}$: user latent factor, where $k$ is the dimension of latent
factors
* $I \in \mathbb{R}^{N \times k}$: item latent factor
* $\mathcal{O}$: Observed interaction set
* $\mathcal{O}^{-}$: Unobserved interaction set

#### Categories of Deep Learning (DL) in RecSys
* **MLP**: stacked layers of non-linear transformations
* **Autoencoder (AE)**: unsupervised model that reconstructs its input data in the output layer
* **CNN**: Use convolution and pooling operations to capture the global and local features
* **RNN**: suitable for modeling sequential data and evolutions
* **Restricted Boltzmann Machine (RBM)**
* **Neural Autoregressive Distribution Estimation(NADE)**: unsupervised neural networks built atop auto-regressive (AR) model and feed-forward neural networks (FFNN)
* **Attentional Models (AM)**
* **Adversarial Networks (AN)**: consists of a discriminator and a generator
* **Deep Reinforcement Learning (DRL)**
* **Deep Hybrid models**: RNN+CNN, AE+CNN, RNN+AE


### Why Deep Learning (DL)?
* In DL, multiple neural building blocks can be composed into a single (gigantic) differentiable function
* DL-based RecSys can be trained end-to-end
* Deep-learning-based methods can take advantage of joint (end-to-end) representation learning.

#### Advantages of DL for RecSys
* Non-linear Transformation
  * Conventional methods such as matrix factorization (MF), factorization machine (FM), sparse linear model are essentially linear models. This limits their expressiveness.
  * In contrast, DL can capture the complex and intricate user item interaction patterns.
  * Can model non-linearity in data with nonlinear activations, e.g. ReLU, Sigmoid, Tanh. 

* Representation Learning
  * Less manual feature engineering: 

* Sequence Modeling
  * Typical applications: next-item/basket prediction and session based recommendation

#### Disadvantages of DL for RecSys
* Interpretability
  * Attention models can alleviate this
* Data requirement is high
* Hyperparameter Tuning is costly
* Deeper models do NOT indicate better performance. The performance of most neural CF models plateaus at three to four layers (*Section 4.3*)
* Also, note that standard ML models such as BPR, MF and CML perform reasonably well when trained with momentum-based gradient descent on interaction-only data


### Training

#### Loss Functions
* weighted square loss (for explicit feedback) 
* binary cross-entropy loss (for implicit feedback)
  * $$\mathcal{L}=-\sum_{(u, i) \in \mathcal{O} \cup \mathcal{O}^{-}} r_{u i} \log \hat{r}_{u i}+\left(1-r_{u i}\right) \log \left(1-\hat{r}_{u i}\right)$$
* Negative Sampling: can be used to reduce the number of training unobserved instances

#### Evaluation


### Representative works

##### Neural CF
A general formula:
* $\hat{r}_{u i}=f\left(U^{T} \cdot s_{u}^{user}, V^{T} \cdot s_{i}^{item} \mid U, V, \theta\right)$
  * $f(\cdot)$ is an MLP.

##### Wide & Deep

* Combines MLP with FM. The goal is to both memorize well and generalize well
* **Wide & Deep** considers low- and high-order feature interactions simultaneously brings additional improvement over considering either alone.
* $P(\hat{r}_{u i}=1 \mid x)=\sigma(\underbrace{W_{\text {w}}^{T}\{x, \phi(x)\}}_{wide}+\underbrace{W_{d}^{T} a^{(l_{f})}}_{deep}+\underbrace{b}_{bias})$
* The **Wide** component is a generalized linear model, which uses handcrafted feature as input
  * $y=W_{\text {w}}^{T}\{x, \phi(x)\}+b$
  * $x$ is the raw input features
  * $\phi(x)$: transformed features
* The **Deep** component concatenates sparse feature embeddings as the input of MLP
  * $\alpha^{(l+1)}=f\left(W_{d}^{(l)} a^{(l)}+b^{(l)}\right)$
  * $f(\cdot)$: Activation function
* Cons: The *Wide* part still needs expertise feature engineering of the inputs

![Wide & Deep](https://deepctr-torch.readthedocs.io/en/latest/_images/WDL.png)


#### Matrix Factorization (MF)
* Introduces hidden variables into CF on top of the co-occurrence matrix.
* Generates a hidden vectors for each user & item that encodes their features
* The hidden vectors are derived from decomposition of the co-occurrence matrix in CF. That's where 
* The idea is simple. In movie recommendation, for the target user $u_a$, MF first finds a set of users $S_a$ who have watched a similar set of movies as $u_a$ did, and then finds the movies watched by these users to recommend to $u_a$.
* Ranking calculation: $\hat{\mathbf{r}}_{\mathrm{ui}}=\mathbf{q}_{\mathrm{i}}^{\mathrm{T}} \mathbf{p}_{\mathrm{u}}$

* SVD
  * Only intended for decomposing square matrices. Not suitable for User-Item matrices

### Factorization Machine (FM)
* Intro to FM ([CSDN](https://blog.csdn.net/itplus/article/details/40534923))
* In FM, feature interaction is defined as the inner product of two feature vectors
* Due to the sparsity of the U-I matrix, many users have not scored an item accordingly, so predicting the degree of a user's preference for an item has become the main task of the recommendation system. 
* The idea of MF is to decompose the U-I matrix into two low-rank dense matrices $P$ and $Q$, where $P$ is the user's hidden factor matrix, and $Q$ is the item's hidden factor matrix. These two matrices are used to predict the user's rating of the item
* Pros(+): Can adapt to second-order feature crossing
* Cons(-): Not readily extensible to higher order (>=3) feature crossing. Curse of dimensionality



##### Product-based Neural Networks (PNN)

* The categorical data is usually represented in a multi-field categorical form, resulting in curse of dimensionality and enormous sparsity
* normally transformed into high-dimensional sparse binary features via one-hot encoding

* Contribution
* starts from an embedding layer without pretraining as in DeepFM
* builds a product layer based on the embedded feature vectors to model the inter-field feature interactions
* further distills the high-order feature patterns with fully connected MLPs.

*  Method
*  The author proposed two types of PNNs with inner and outer product operations in the product layer
* $\boldsymbol{l}_{1}=\operatorname{relu}\left(\boldsymbol{l}_{z}+\boldsymbol{l}_{p}+\boldsymbol{b}_{1}\right)$

* Loss: Binary Cross Entropy

##### DeepFM

* Motivation: most works lack implicit feature interactions.
  * e.g., people usually download apps for food delivery at meal-time, suggesting that the order-2 interaction between app category and timestamp can be used to model CTR

* An advanced version of Wide & Deep
* Eliminates the need of pre-training by FM
* The network can be trained jointly in an end-to-end manner. 

* Does NOT require excessive feature engineering, compared to wide & deep model, which requires feature engineering in its *wide* part.
* **xDeepFM** is a variation proposed by Lian et al. The explicit high-order feature interactions are learned via a compressed interaction network

##### DSPR

* Each user $x_u$ and item $x_i$ are represented by tag annotations
* $\underset{U, V}{\arg \min } \ell(R, U, V)+\beta\left(\|U\|_{F}^{2}+\|V\|_{F}^{2}\right)+\gamma \mathcal{L}(X, U)+\delta \mathcal{L}(Y, V)$
* $\beta, \gamma, \delta$ are hyperparameters to balance the influences of the three components

##### DeepCF

* a general framework for unifying deep learning with collaborative filtering


##### DeepCoNN
adopts two parallel CNNs to model user behaviors and item properties from review texts.


##### CNN for Recommendation
* Cons: biased to the interactions between neighboring features

##### GRU4Rec
* a session-based recommendation model. A variation of RNN for Recommendation
* Input: session state 
* Ranking Loss:

$$\mathcal{L}_{s}=\frac{1}{S} \sum_{j=1}^{S} \sigma\left(\hat{r}_{s j}-\hat{r}_{s i}\right)+\sigma\left(\hat{r}_{s j}^{2}\right)$$

* $s$ indicates session
* $\hat{r}_{s i}$ and $\hat{r}_{s j}$ are the scores on negative item $i$ and positive item $j$ at session $s$

##### AutoRec
Structurally similar to Word2Vec. Applies DNN to recommender systems

* Pros: Simplicity
* Cons: Hard for feature crossing

##### DIEN
* Integrates sequential modeling into recommendation. DIEM can model the evolution of user interests

##### Deep Reinforcement Network (DRN)
* Integrates RL into recommendation

##### Deep Crossing

* The model was applied in Ads recommendation in Bing
* Motivation:
  * How to convert sparse features into dense ones for better training?
  * How to perform automatic feature crossing
* Pros(+): No manual feature engineering. The original features are directly fed into the neural netowrks after feature crossing.

### Future Direction

* Multi-step inference and reasoning: reason over a user's social prole, purchases etc., reasoning over multiple modalities to recommend a product

### Related Works (Survey Papers)

Survey papers on RecSys
* [A Survey on Knowledge Graph-Based Recommender Systems](https://arxiv.org/abs/2003.00911) by USTC, Baidu and MSRA
* [Cross domain recommender systems: a systematic literature review](https://dl.acm.org/doi/abs/10.1145/3073565)

General:
* [Deep Learning](http://www.deeplearningbook.org) by Ian Goodfellow


### Remaining questions
* Negative sampling
* Solution of cold start
  

---

## Deep Learning Recommender System 
* Author of the book: Zhe Wang.
* Please note that the book is not written in English, but I believe there will soon be one ...

### Chapter 2
* Despite the popularity of deep learning, traditional methods like collaborative filtering, logistic regression, factorization machines are still heavily used due to their robustness, interpretability, low hardware requirements, and the ease of training and deployment, etc.

* Pros(+): Intuitive. The preference of a person depends on the preference of other similar individuals 
  * Cons(-):

#### 2.1 Evolution of traditional methods

* Collaborative Filtering
* Logistic Regression

#### 2.2 Collaborative Filtering (CF)

* [Definition from Wikipedia](https://en.wikipedia.org/wiki/Collaborative_filtering): a method of making automatic predictions (filtering) about the interests of a user by collecting preferences from several similar users (collaborative).
  * Collaborative means decision making using social (crowd) behaviors
* An introduction of CF ([zhihu](https://zhuanlan.zhihu.com/p/80069337))
* User feedbacks can be expressed using a directed graph, with the user as source, the item as destination, and the user feedback (1 for "like", -1 for "dislike", and "0" for no information) as edge attribute
* User similarity is important in CF, which is usualy calculated using cosine similarity
* Compared with cosine similarity, [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient), a.k.a. Pearson's $r$, can correct the bias in user evaluation
  * $\operatorname{sim}(i, j)=\frac{\sum_{\mathrm{p} \epsilon P}(R_{\mathrm{i}, \mathrm{p}}-\bar{R}_{\mathrm{i}})(R_{\mathrm{j}, \mathrm{p}}-\bar{R}_{\mathrm{j}})}{\sqrt{\sum_{\mathrm{p} \epsilon P}(R_{\mathrm{i}, \mathrm{p}}-\bar{R}_{\mathrm{i}})^{2}} \sqrt{\sum_{\mathrm{p} \epsilon P}(R_{\mathrm{j}, \mathrm{p}}-\bar{R}_{\mathrm{j}})^{2}}}$
  * $P$ is the set of items jointly evaluated by user $i$ and $j$
  * $R_{i,p}$ is the rating of user $a$ to item $p$
  * $\bar{R}_i$ is the average item ratings given by user $i$ for ALL items. (Same for $\bar{R}_j$  and user $j$)

* User-based CF
  * Compute similarity between users and active users, and use similar users' ratings as prediction

  * Features stronger "Social characteristics." Users can update their list of recommendations immediately from like-minded users.
  * Pros(+): Intuitive. The preference of a person depends on the preference of other similar individuals 
  * Cons(-): Not suitable for applications with sparse positive feedbacks, e.g., hotel reservation

* Item-based CF
  * Compute similarity between items, and predict similar rating to similar items that the active user has rated before
  * Steps: (Assume $M$ users and $N$ items)
  * Construct an $M \times N$ co-occurrence matrix based on interaction history
  * Calculate the $N \times N$ item similarity matrix through the column vector in the co-occurrence matrix
  * Get the items with positive feedbacks in users' interaction history
  * Find the top $k$ items w.r.t. the item with positive feedbacks, and rank them

### Evaluation Metrics
* 

### Dataset


### Chapter 3
* NeuralCF: 









